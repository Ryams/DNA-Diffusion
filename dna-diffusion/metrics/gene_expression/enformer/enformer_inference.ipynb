{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinellolab/DNA-Diffusion/blob/enformer-implementation/dna-diffusion/metrics/gene_expression/enformer/enformer_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koT2b2y2ZWQw"
      },
      "source": [
        "# Enformer inference notbook\n",
        "This notebook executes all functionality related to preprocessing sequence data and performing Enformer inference. As input we need to provide a gene and this code handles fetching the sequence of the gene. Then we need to make sure we extend the window 200kb around the transcription start site because Enformer only accepts 200kb inputs. Then we copy-and-paste our generated regulatory sequence instead of one of the regulatory elements of the gene we are considering and run the inference. At the moment, we do not used novel generated sequences yet as the DNA diffusion integration is not completed yet. Instead, we use the ABC data for the time being, which is a dataset containing regulatory sequences from the human genome.\n",
	@@ -40,27 +25,28 @@
        "*   Import supplementary data table 2 from Enformer paper in order to get the cell types and genomic track type.\n",
        "*   Perform sanity check on DNA diff test data see: https://discord.com/channels/850068776544108564/1024646567833112656/1055581251483996210`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I18uA7c2q17W",
        "outputId": "70a8f7aa-98a5-4a86-f71b-2bb52fd1810e"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/gdrive/\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
	@@ -71,41 +57,27 @@
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3zI2-6PYWXZ"
      },
      "source": [
        "Install all dependencies. When installed set setup to false to prevent time consuming install checks when running entire notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCAA-1Dx5atV",
        "outputId": "73ddf0fd-16ae-4a9c-afea-2a9d9f6c417f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
	@@ -246,13 +218,33 @@
            "Successfully installed discrete-key-value-bottleneck-pytorch-0.0.6 enformer-pytorch-0.5.6 torchmetrics-0.11.0 vector-quantize-pytorch-0.10.14\n"
          ]
        }
      ],
      "source": [
        "setup = True\n",
        "\n",
        "if setup:\n",
        "  %pip install transformers\n",
        "  %pip install einops \n",
        "  %pip install polars\n",
        "  %pip install pyfaidx\n",
        "  %pip install mygene\n",
        "  !apt-get install bedtools\n",
        "  %pip install pybedtools\n",
        "  %pip install biopython\n",
        "  %pip install enformer-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H28e39nq7Mc_"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import torch\n",
        " \n",
        "ROOT_DIR = '/content/gdrive/MyDrive/'\n",
	@@ -263,15 +255,15 @@
        "from enformer_lucidrains_pytorch.enformer_pytorch import Enformer\n",
        "from dataloader import EnformerDataLoader\n",
        "from utils import inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qunwrae8Xr5_"
      },
      "outputs": [],
      "source": [
        "class EnformerInference:\n",
        "    def __init__(self, data_path: str, model_path=\"EleutherAI/enformer-official-rough\"):\n",
	@@ -289,31 +281,20 @@
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x.to(self.device))\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbzz4Yg-iadf"
      },
      "source": [
        "Do not forget to set `Runtime > Change runtime type > GPU` in order to do inference via GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
	@@ -322,19 +303,18 @@
        "id": "Y_Yk7AHQYQHK",
        "outputId": "3c28168d-1493-48d9-8de1-830d3f21e309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using NVIDIA GPU\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
	@@ -353,7 +333,101 @@
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 14.76 GiB total capacity; 13.42 GiB already allocated; 7.75 MiB free; 13.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ],
      "source": [
        "data_path = \"abc_data/K562.PositivePredictions.txt\"\n",
        "model = EnformerInference(data_path)\n",
        "one_hot_seqs = model.data.fetch_sequence()  # this is a dictionary with key being Ensembl ID|Gene Name and the value\n",
        "# being the one hot encoded sequence as a torch.Tensor\n",
        "inference(one_hot_seqs, model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f'{ROOT_DIR}{PROJ_DIR}outputs/ENSG00000205639_MFSD2B.pkl', 'rb') as f:\n",
        "     output = torch.load(f, map_location=torch.device('cpu'))\n",
        "f.close()\n",
        "output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output['human']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output['human'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_dataframe(table_path, output):\n",
        "  df = pd.read_excel(table_path, \"Supplementary Table 2\", index_col = None, na_values = ['NA'], usecols = \"I:J\")\n",
        "  #column I: assay_type\n",
        "  #column J: target\n",
        "\n",
        "  #add enformer output to dataframe\n",
        "  tracks_output = pd.DataFrame([[track] for track in output.T.detach().numpy()])\n",
        "  df['output'] = tracks_output\n",
        "\n",
        "  #remove target information from target column\n",
        "  targets = df['target'].str.split(pat=\"/\", n=1, expand=True)\n",
        "  df['target'] = targets[1]\n",
        "  return df\n",
        "\n",
        "df = create_dataframe(\"/content/gdrive/MyDrive/enformer/41592_2021_1252_MOESM3_ESM.xlsx\", output['human'])\n",
        "print(df.head())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyNyPESG1IBKl+wWp6J8B2mp",
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "ebbca54691d61843c0a04253fcd790a2bc545e11985b7cc4dd8a14aab0b5083b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
