{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install and download"
      ],
      "metadata": {
        "id": "0dosR33d4Q5d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VDoTvvQ392y",
        "outputId": "2c38d2da-7edb-4a44-b195-c62da6c31352"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing"
      ],
      "metadata": {
        "id": "tzadEFWb4aNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install einops\n",
        "!pip install pytorch_lightning\n",
        "import os; os.getpid()\n",
        "from scipy.stats import zscore\n",
        "import torch\n",
        "import copy\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "import torch\n",
        "from IPython.display import display\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import make_grid\n",
        "from torchvision.utils import save_image\n",
        "from IPython.display import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.nn.modules.activation import ReLU\n",
        "from torch.optim import Adam\n",
        "from tqdm import tqdm_notebook\n",
        "from torchvision.utils import save_image\n",
        "import matplotlib\n",
        "import math\n",
        "from inspect import isfunction\n",
        "from functools import partial\n",
        "import scipy\n",
        "from scipy.special import rel_entr\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from einops import rearrange\n",
        "from torch import nn, einsum\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.animation as animation\n",
        "import matplotlib.image as mpimg\n",
        "import glob\n",
        "from PIL import Image\n",
        "import pytorch_lightning as pl \n",
        "import imageio\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import grad as torch_grad\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "FfG45jNd4bof",
        "outputId": "b17740ad-7c41-4b6e-9a5e-4877fc055af4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[K     |████████████████████████████████| 41 kB 332 kB/s \n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.8.1-py3-none-any.whl (798 kB)\n",
            "\u001b[K     |████████████████████████████████| 798 kB 12.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2022.10.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.21.6)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.9.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: torch>=1.9.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.12.1+cu113)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.1.1)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.10.2-py3-none-any.whl (529 kB)\n",
            "\u001b[K     |████████████████████████████████| 529 kB 64.3 MB/s \n",
            "\u001b[?25hCollecting lightning-utilities==0.3.*\n",
            "  Downloading lightning_utilities-0.3.0-py3-none-any.whl (15 kB)\n",
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>2021.06.0->pytorch_lightning) (3.8.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (22.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (0.13.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (1.3.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch_lightning) (4.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (2.14.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.3.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.38.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.50.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.19.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (5.2.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]>2021.06.0->pytorch_lightning) (2022.9.24)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (3.2.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire->lightning-utilities==0.3.*->pytorch_lightning) (2.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115940 sha256=bedf2ba239187b0491c91b82645774b477f802886ae5800dda9bd95100d8e80a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: fire, torchmetrics, lightning-utilities, pytorch-lightning\n",
            "Successfully installed fire-0.4.0 lightning-utilities-0.3.0 pytorch-lightning-1.8.1 torchmetrics-0.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset "
      ],
      "metadata": {
        "id": "OBANVwrV5N8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceDatasetBase(Dataset):\n",
        "    def __init__(self, data_path, sequence_length=200, sequence_encoding=\"polar\", sequence_transform=None, cell_type_transform=None):\n",
        "        super().__init__()\n",
        "        self.data = pd.read_csv(data_path, sep=\"\\t\")\n",
        "        self.sequence_length = sequence_length\n",
        "        self.sequence_encoding = sequence_encoding\n",
        "        self.sequence_transform = sequence_transform\n",
        "        self.cell_type_transform = cell_type_transform\n",
        "        self.alphabet = [\"A\", \"C\", \"T\", \"G\"]\n",
        "        self.check_data_validity()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # Iterating through DNA sequences from dataset and one-hot encoding all nucleotides\n",
        "        current_seq = self.data[\"raw_sequence\"][index]\n",
        "        if 'N' not in current_seq: \n",
        "            X_seq = self.encode_sequence(current_seq, encoding=self.sequence_encoding)\n",
        "            \n",
        "            # Reading cell component at current index\n",
        "            X_cell_type = self.data[\"component\"][index]\n",
        "            \n",
        "            if self.sequence_transform is not None:\n",
        "                X_seq = self.sequence_transform(X_seq)\n",
        "            if self.cell_type_transform is not None:\n",
        "                X_cell_type = self.cell_type_transform(X_cell_type)\n",
        "\n",
        "            return X_seq, X_cell_type\n",
        "\n",
        "    def check_data_validity(self):\n",
        "        \"\"\"\n",
        "        Checks if the data is valid.\n",
        "        \"\"\"\n",
        "        if not set(\"\".join(self.data[\"raw_sequence\"])).issubset(set(self.alphabet)):\n",
        "            raise ValueError(f\"Sequence contains invalid characters.\")\n",
        "\n",
        "        uniq_raw_seq_len = self.data[\"raw_sequence\"].str.len().unique()\n",
        "        if len(uniq_raw_seq_len) != 1 or uniq_raw_seq_len[0] != self.sequence_length:\n",
        "            raise ValueError(f\"The sequence length does not match the data.\")\n",
        "\n",
        "    def encode_sequence(self, seq, encoding):\n",
        "        \"\"\"\n",
        "        Encodes a sequence using the given encoding scheme (\"polar\", \"onehot\", \"ordinal\").\n",
        "        \"\"\"\n",
        "        if encoding == \"polar\":\n",
        "            seq = self.one_hot_encode(seq).T\n",
        "            seq[seq == 0] = -1\n",
        "        elif encoding == \"onehot\":\n",
        "            seq = self.one_hot_encode(seq).T\n",
        "        elif encoding == \"ordinal\":\n",
        "            seq = np.array([self.alphabet.index(n) for n in seq])\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoding scheme: {encoding}\")\n",
        "        return seq\n",
        "\n",
        "    # Function for one hot encoding each line of the sequence dataset\n",
        "    def one_hot_encode(self, seq):\n",
        "        \"\"\"\n",
        "        One-hot encoding a sequence\n",
        "        \"\"\"\n",
        "        seq_len = len(seq)\n",
        "        seq_array = np.zeros((self.sequence_length, len(self.alphabet)))\n",
        "        for i in range(seq_len):\n",
        "            seq_array[i, self.alphabet.index(seq[i])] = 1\n",
        "        return seq_array\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H-VwhLHE5PYE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceDatasetTrain(SequenceDatasetBase):\n",
        "    def __init__(self, data_path=\"\", **kwargs):\n",
        "        super().__init__(data_path=data_path, **kwargs)\n",
        "\n",
        "class SequenceDatasetValidation(SequenceDatasetBase):\n",
        "    def __init__(self, data_path=\"\", **kwargs):\n",
        "        super().__init__(data_path=data_path, **kwargs)\n",
        "\n",
        "class SequenceDatasetTest(SequenceDatasetBase):\n",
        "    def __init__(self, data_path=\"\", **kwargs):\n",
        "        super().__init__(data_path=data_path, **kwargs)\n",
        "\n",
        "\n",
        "class SequenceDataModule(pl.LightningDataModule):\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_path=None,\n",
        "        val_path=None,\n",
        "        test_path=None,\n",
        "        sequence_length=200,\n",
        "        sequence_encoding=\"polar\",\n",
        "        sequence_transform=None,\n",
        "        cell_type_transform=None,\n",
        "        batch_size=None,\n",
        "        num_workers=1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.datasets = dict()\n",
        "        self.train_dataloader, self.val_dataloader, self.test_dataloader = None, None, None\n",
        "\n",
        "        if train_path:\n",
        "            self.datasets[\"train\"] = train_path\n",
        "            self.train_dataloader = self._train_dataloader\n",
        "\n",
        "        if val_path:\n",
        "            self.datasets[\"validation\"] = val_path\n",
        "            self.val_dataloader = self._val_dataloader\n",
        "\n",
        "        if test_path:\n",
        "            self.datasets[\"test\"] = test_path\n",
        "            self.test_dataloader = self._test_dataloader\n",
        "\n",
        "        self.sequence_length = sequence_length\n",
        "        self.sequence_encoding = sequence_encoding\n",
        "        self.sequence_transform = sequence_transform\n",
        "        self.cell_type_transform = cell_type_transform\n",
        "        self.batch_size = batch_size\n",
        "        self.num_workers = num_workers\n",
        "\n",
        "    def setup(self):\n",
        "        if \"train\" in self.datasets:\n",
        "            self.train_data = SequenceDatasetTrain(\n",
        "                data_path=self.datasets[\"train\"],\n",
        "                sequence_length=self.sequence_length,\n",
        "                sequence_encoding=self.sequence_encoding,\n",
        "                sequence_transform=self.sequence_transform,\n",
        "                cell_type_transform=self.cell_type_transform\n",
        "            )\n",
        "        if \"validation\" in self.datasets:\n",
        "            self.val_data = SequenceDatasetValidation(\n",
        "                data_path=self.datasets[\"validation\"],\n",
        "                sequence_length=self.sequence_length,\n",
        "                sequence_encoding=self.sequence_encoding,\n",
        "                sequence_transform=self.sequence_transform,\n",
        "                cell_type_transform=self.cell_type_transform\n",
        "            )\n",
        "        if \"test\" in self.datasets:\n",
        "            self.test_data = SequenceDatasetTest(\n",
        "                data_path=self.datasets[\"test\"],\n",
        "                sequence_length=self.sequence_length,\n",
        "                sequence_encoding=self.sequence_encoding,\n",
        "                sequence_transform=self.sequence_transform,\n",
        "                cell_type_transform=self.cell_type_transform\n",
        "            )\n",
        "\n",
        "    def _train_dataloader(self):\n",
        "        return DataLoader(self.train_data,\n",
        "                          self.batch_size, \n",
        "                          shuffle=True, \n",
        "                          num_workers=self.num_workers, \n",
        "                          pin_memory=True)\n",
        "\n",
        "    def _val_dataloader(self):\n",
        "        return DataLoader(self.val_data,\n",
        "                          self.batch_size, \n",
        "                          shuffle=True,\n",
        "                          num_workers=self.num_workers,\n",
        "                          pin_memory=True)\n",
        "\n",
        "    def _test_dataloader(self):\n",
        "        return DataLoader(self.test_data,\n",
        "                          self.batch_size, \n",
        "                          shuffle=True, \n",
        "                          num_workers=self.num_workers, \n",
        "                          pin_memory=True)"
      ],
      "metadata": {
        "id": "13a5Drpd5VpF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the data"
      ],
      "metadata": {
        "id": "vA9bReW8795S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget https://www.dropbox.com/s/db6up7c0d4jwdp4/train_all_classifier_WM20220916.csv.gz\n",
        "! gunzip -d /content/train_all_classifier_WM20220916.csv.gz"
      ],
      "metadata": {
        "id": "TmErfC4K77bj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d36f0437-bc0a-4bf4-c03d-93eb12fdded6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-13 15:25:59--  https://www.dropbox.com/s/db6up7c0d4jwdp4/train_all_classifier_WM20220916.csv.gz\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.2.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.2.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/db6up7c0d4jwdp4/train_all_classifier_WM20220916.csv.gz [following]\n",
            "--2022-11-13 15:26:00--  https://www.dropbox.com/s/raw/db6up7c0d4jwdp4/train_all_classifier_WM20220916.csv.gz\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdf184ab69171cb593834733b47.dl.dropboxusercontent.com/cd/0/inline/BwpJaLiPROHOtErUSWehuRtgE188NF5Gff-ZHmPLIoRAPm313DS-iajmccjkGblZeEZR1pzfvcAlBoKhIdzgaAaJaeg6Hz2IBRcbQaa6AhhshUnFX6RvsyT-a1FVvt7ushC5u3CWiUSpIe8peQF1hy1ToCGYMR741MU5W5ZdK4u8pA/file# [following]\n",
            "--2022-11-13 15:26:01--  https://ucdf184ab69171cb593834733b47.dl.dropboxusercontent.com/cd/0/inline/BwpJaLiPROHOtErUSWehuRtgE188NF5Gff-ZHmPLIoRAPm313DS-iajmccjkGblZeEZR1pzfvcAlBoKhIdzgaAaJaeg6Hz2IBRcbQaa6AhhshUnFX6RvsyT-a1FVvt7ushC5u3CWiUSpIe8peQF1hy1ToCGYMR741MU5W5ZdK4u8pA/file\n",
            "Resolving ucdf184ab69171cb593834733b47.dl.dropboxusercontent.com (ucdf184ab69171cb593834733b47.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6021:15::a27d:410f\n",
            "Connecting to ucdf184ab69171cb593834733b47.dl.dropboxusercontent.com (ucdf184ab69171cb593834733b47.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bwp92zg6yhkL81Z-K3tLzTFNTBXghbvcuGXsHYpVZuyEIbVGJc9pcwLaihbIPH-wL-QEj7QOMSfmU6yZhagHRj6zSXaHl4yKA8TCnFEg1Nt7Aspc_hJ9SHs5hJn79VUNO245P-VSsVMWRfmRdAZhCwdDpoUWVRQZjKii-uiOLaSSFL6y7pxFSLXs5ZNsC228a7oPSzJmSRB4JrPBGDJNOrbl24PsDEKkuYiGzZLYn_t2L2aty8Tf7WWhW-0yChAFiPWDvbmeUgoshgwL-eKwMDdQuEGl9ltBZnD4gOPh7W4z5ti8ZXt0QuntlvQv1Bb499FNxBt-TSBrGTkko8oJ7F8u2kelUQ6JcBOsruN9baA4tR6zT3rbyhHrH7ETiRHkC2dCBcq3aPLtNPZRFeCGgoCa-ukRb-zXTBjQPOj7ybj4zQ/file [following]\n",
            "--2022-11-13 15:26:01--  https://ucdf184ab69171cb593834733b47.dl.dropboxusercontent.com/cd/0/inline2/Bwp92zg6yhkL81Z-K3tLzTFNTBXghbvcuGXsHYpVZuyEIbVGJc9pcwLaihbIPH-wL-QEj7QOMSfmU6yZhagHRj6zSXaHl4yKA8TCnFEg1Nt7Aspc_hJ9SHs5hJn79VUNO245P-VSsVMWRfmRdAZhCwdDpoUWVRQZjKii-uiOLaSSFL6y7pxFSLXs5ZNsC228a7oPSzJmSRB4JrPBGDJNOrbl24PsDEKkuYiGzZLYn_t2L2aty8Tf7WWhW-0yChAFiPWDvbmeUgoshgwL-eKwMDdQuEGl9ltBZnD4gOPh7W4z5ti8ZXt0QuntlvQv1Bb499FNxBt-TSBrGTkko8oJ7F8u2kelUQ6JcBOsruN9baA4tR6zT3rbyhHrH7ETiRHkC2dCBcq3aPLtNPZRFeCGgoCa-ukRb-zXTBjQPOj7ybj4zQ/file\n",
            "Reusing existing connection to ucdf184ab69171cb593834733b47.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22394917 (21M) [application/octet-stream]\n",
            "Saving to: ‘train_all_classifier_WM20220916.csv.gz’\n",
            "\n",
            "train_all_classifie 100%[===================>]  21.36M  28.7MB/s    in 0.7s    \n",
            "\n",
            "2022-11-13 15:26:02 (28.7 MB/s) - ‘train_all_classifier_WM20220916.csv.gz’ saved [22394917/22394917]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = pd.read_csv('/content/train_all_classifier_WM20220916.csv', sep=\"\\t\")\n",
        "data_test"
      ],
      "metadata": {
        "id": "iv27XSci8CA6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "outputId": "9d74c12d-9bb4-4cc0-f18b-95362a5155d7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Unnamed: 0 seqname      start        end  DHS_width     summit  \\\n",
              "0          1241720   chr16   68843660   68843880        220   68843790   \n",
              "1          2251755    chr3  143634500  143634720        220  143634610   \n",
              "2          3136863    chr7  156928220  156928441        221  156928330   \n",
              "3          2234828    chr3  130738277  130738580        303  130738460   \n",
              "4          3060272    chr7   95784860   95785160        300   95785010   \n",
              "...            ...     ...        ...        ...        ...        ...   \n",
              "159995     2953063    chr7    2602077    2602454        377    2602280   \n",
              "159996     3205276    chr8   52933200   52933440        240   52933320   \n",
              "159997     2024911   chr22   20858820   20859336        516   20859160   \n",
              "159998     2598954    chr5   72442240   72442480        240   72442350   \n",
              "159999     1887625   chr20    3183993    3184220        227    3184090   \n",
              "\n",
              "        total_signal  numsamples  numpeaks        C1  ...       C10       C11  \\\n",
              "0         122.770678          61        61  0.101076  ...  0.005301  0.016703   \n",
              "1           0.780678           1         1  0.000000  ...  0.000000  0.000000   \n",
              "2         145.069295          32        32  0.000000  ...  0.046526  0.002177   \n",
              "3          13.140313          10        10  0.000531  ...  0.000000  0.000000   \n",
              "4          17.523798           7         7  0.000000  ...  0.011486  0.000000   \n",
              "...              ...         ...       ...       ...  ...       ...       ...   \n",
              "159995      1.058019           2         2  0.000000  ...  0.018323  0.000000   \n",
              "159996     11.820170           2         2  0.002420  ...  0.000000  0.000000   \n",
              "159997      5.475445           4         4  0.004008  ...  0.000000  0.000000   \n",
              "159998      4.672712           5         5  0.000000  ...  0.000000  0.000714   \n",
              "159999      2.297204           4         4  0.000000  ...  0.000000  0.000000   \n",
              "\n",
              "             C12       C13       C14       C15       C16  \\\n",
              "0       0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "1       0.000000  0.008636  0.000000  0.000000  0.000000   \n",
              "2       0.008559  0.000000  0.106442  0.000000  0.000000   \n",
              "3       0.000000  0.000370  0.000000  0.043161  0.000000   \n",
              "4       0.000000  0.000000  0.036866  0.000000  0.000000   \n",
              "...          ...       ...       ...       ...       ...   \n",
              "159995  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "159996  0.000000  0.000000  0.000000  0.000282  0.000000   \n",
              "159997  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "159998  0.003835  0.000000  0.000000  0.000000  0.000000   \n",
              "159999  0.000000  0.000000  0.000000  0.000000  0.025817   \n",
              "\n",
              "                                             raw_sequence  component  \\\n",
              "0       GAGGCATTGAAGCTGCTGCTGAGCCCGGGAGGTGAGAGGACGCATC...          0   \n",
              "1       CTCTCCAACTTTTTCCCTGAGTATTGCCAGCACACTTTTAATCTCC...         12   \n",
              "2       CTTCCTGATAAGATCTCAGGAGCTGGGCAAGTGGCTCAAGTATGTG...         13   \n",
              "3       TGAGGAACATAAGCACATAAAATATAATCTAGAAGTTGGTGCTGAG...         14   \n",
              "4       CCAGGTTCTGCCATTCACTTGGGGCCAGCATAAACAAGGGGGCAGG...         13   \n",
              "...                                                   ...        ...   \n",
              "159995  CAGAGCTCACTGGACCGGGAAGTGAGGGGAGGGCATCCCAGCAGAG...          9   \n",
              "159996  CAGTAAAAGTTTATCACCAGCAGAATGCACTTAAAATATTAAGTGA...          0   \n",
              "159997  GCAGTGGGGCTCCTCCTTCTGTTTCCCAGACCGAGAGCCGCGCCGG...          5   \n",
              "159998  GAAGTCTCTGGGAAGTGTCCTGGAAGCCACAGAAATGGTGAGTTCT...          3   \n",
              "159999  TTCAGTACTCTGTTTTTCAAACTGTAGGTCAAGATCTGTTAGTGAG...         15   \n",
              "\n",
              "        proportion  \n",
              "0         0.767372  \n",
              "1         0.869445  \n",
              "2         0.585111  \n",
              "3         0.961271  \n",
              "4         0.762448  \n",
              "...            ...  \n",
              "159995    0.928153  \n",
              "159996    0.664603  \n",
              "159997    0.707226  \n",
              "159998    0.809984  \n",
              "159999    1.000000  \n",
              "\n",
              "[160000 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1d0fb4aa-4a97-4060-ac91-f657eb33cf94\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>seqname</th>\n",
              "      <th>start</th>\n",
              "      <th>end</th>\n",
              "      <th>DHS_width</th>\n",
              "      <th>summit</th>\n",
              "      <th>total_signal</th>\n",
              "      <th>numsamples</th>\n",
              "      <th>numpeaks</th>\n",
              "      <th>C1</th>\n",
              "      <th>...</th>\n",
              "      <th>C10</th>\n",
              "      <th>C11</th>\n",
              "      <th>C12</th>\n",
              "      <th>C13</th>\n",
              "      <th>C14</th>\n",
              "      <th>C15</th>\n",
              "      <th>C16</th>\n",
              "      <th>raw_sequence</th>\n",
              "      <th>component</th>\n",
              "      <th>proportion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1241720</td>\n",
              "      <td>chr16</td>\n",
              "      <td>68843660</td>\n",
              "      <td>68843880</td>\n",
              "      <td>220</td>\n",
              "      <td>68843790</td>\n",
              "      <td>122.770678</td>\n",
              "      <td>61</td>\n",
              "      <td>61</td>\n",
              "      <td>0.101076</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005301</td>\n",
              "      <td>0.016703</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>GAGGCATTGAAGCTGCTGCTGAGCCCGGGAGGTGAGAGGACGCATC...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.767372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2251755</td>\n",
              "      <td>chr3</td>\n",
              "      <td>143634500</td>\n",
              "      <td>143634720</td>\n",
              "      <td>220</td>\n",
              "      <td>143634610</td>\n",
              "      <td>0.780678</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008636</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>CTCTCCAACTTTTTCCCTGAGTATTGCCAGCACACTTTTAATCTCC...</td>\n",
              "      <td>12</td>\n",
              "      <td>0.869445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3136863</td>\n",
              "      <td>chr7</td>\n",
              "      <td>156928220</td>\n",
              "      <td>156928441</td>\n",
              "      <td>221</td>\n",
              "      <td>156928330</td>\n",
              "      <td>145.069295</td>\n",
              "      <td>32</td>\n",
              "      <td>32</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.046526</td>\n",
              "      <td>0.002177</td>\n",
              "      <td>0.008559</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106442</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>CTTCCTGATAAGATCTCAGGAGCTGGGCAAGTGGCTCAAGTATGTG...</td>\n",
              "      <td>13</td>\n",
              "      <td>0.585111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2234828</td>\n",
              "      <td>chr3</td>\n",
              "      <td>130738277</td>\n",
              "      <td>130738580</td>\n",
              "      <td>303</td>\n",
              "      <td>130738460</td>\n",
              "      <td>13.140313</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000370</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.043161</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>TGAGGAACATAAGCACATAAAATATAATCTAGAAGTTGGTGCTGAG...</td>\n",
              "      <td>14</td>\n",
              "      <td>0.961271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3060272</td>\n",
              "      <td>chr7</td>\n",
              "      <td>95784860</td>\n",
              "      <td>95785160</td>\n",
              "      <td>300</td>\n",
              "      <td>95785010</td>\n",
              "      <td>17.523798</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011486</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.036866</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>CCAGGTTCTGCCATTCACTTGGGGCCAGCATAAACAAGGGGGCAGG...</td>\n",
              "      <td>13</td>\n",
              "      <td>0.762448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159995</th>\n",
              "      <td>2953063</td>\n",
              "      <td>chr7</td>\n",
              "      <td>2602077</td>\n",
              "      <td>2602454</td>\n",
              "      <td>377</td>\n",
              "      <td>2602280</td>\n",
              "      <td>1.058019</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.018323</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>CAGAGCTCACTGGACCGGGAAGTGAGGGGAGGGCATCCCAGCAGAG...</td>\n",
              "      <td>9</td>\n",
              "      <td>0.928153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159996</th>\n",
              "      <td>3205276</td>\n",
              "      <td>chr8</td>\n",
              "      <td>52933200</td>\n",
              "      <td>52933440</td>\n",
              "      <td>240</td>\n",
              "      <td>52933320</td>\n",
              "      <td>11.820170</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.002420</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000282</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>CAGTAAAAGTTTATCACCAGCAGAATGCACTTAAAATATTAAGTGA...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.664603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159997</th>\n",
              "      <td>2024911</td>\n",
              "      <td>chr22</td>\n",
              "      <td>20858820</td>\n",
              "      <td>20859336</td>\n",
              "      <td>516</td>\n",
              "      <td>20859160</td>\n",
              "      <td>5.475445</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0.004008</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>GCAGTGGGGCTCCTCCTTCTGTTTCCCAGACCGAGAGCCGCGCCGG...</td>\n",
              "      <td>5</td>\n",
              "      <td>0.707226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159998</th>\n",
              "      <td>2598954</td>\n",
              "      <td>chr5</td>\n",
              "      <td>72442240</td>\n",
              "      <td>72442480</td>\n",
              "      <td>240</td>\n",
              "      <td>72442350</td>\n",
              "      <td>4.672712</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000714</td>\n",
              "      <td>0.003835</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>GAAGTCTCTGGGAAGTGTCCTGGAAGCCACAGAAATGGTGAGTTCT...</td>\n",
              "      <td>3</td>\n",
              "      <td>0.809984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159999</th>\n",
              "      <td>1887625</td>\n",
              "      <td>chr20</td>\n",
              "      <td>3183993</td>\n",
              "      <td>3184220</td>\n",
              "      <td>227</td>\n",
              "      <td>3184090</td>\n",
              "      <td>2.297204</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.025817</td>\n",
              "      <td>TTCAGTACTCTGTTTTTCAAACTGTAGGTCAAGATCTGTTAGTGAG...</td>\n",
              "      <td>15</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>160000 rows × 28 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1d0fb4aa-4a97-4060-ac91-f657eb33cf94')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1d0fb4aa-4a97-4060-ac91-f657eb33cf94 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1d0fb4aa-4a97-4060-ac91-f657eb33cf94');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encode data and test loader looping"
      ],
      "metadata": {
        "id": "O2DaM-Ba8STa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encode_data = SequenceDatasetBase(data_path=\"./train_all_classifier_WM20220916.csv\",\n",
        "                                                      sequence_length=200, sequence_encoding=\"polar\",\n",
        "                                                      sequence_transform=None, cell_type_transform=None)"
      ],
      "metadata": {
        "id": "mKOXJBJk8ToD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "it = iter(encode_data)\n",
        "s = next(it) # seems to be a tuple with second dimension empty and first dimension of size (4,200)\n",
        "print(s[0].shape)"
      ],
      "metadata": {
        "id": "E8MFFiXcDMXR",
        "outputId": "ddd22e2b-d750-4031-9c20-13393d2231c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WGAN-GP"
      ],
      "metadata": {
        "id": "vlSQ0FSP5w_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.biorxiv.org/content/10.1101/2022.07.26.501466v1.full.pdf#page=10\n",
        "Generative Adversarial Network\n",
        "To train a GAN model, we used Wasserstein GAN architecture with gradient penalty similar to earlier work. \n",
        "The model consists of two parts; generator and discriminator. Generator takes noise as input (size is 128), \n",
        "followed by a dense layer with 64,000 (500 * 128) units with ELU activation, a reshape layer (500, 128), \n",
        "a convolution tower of 5 convolution blocks with skip connections, \n",
        "a 1D convolution layer with 4 filters with kernel width 1, and finally a SOFTMAX activation layer. \n",
        "The output of the generator is a 500 × 4 matrix, which represents one-hot encoded DNA sequence. \n",
        "\n",
        "Discriminator takes 500 bp one-hot encoded DNA sequence as input (real or fake), \n",
        "followed by a 1D convolution layer with 128 filters with kernel width 1, \n",
        "a convolution tower of 5 convolution blocks with skip connections, a flatten layer, \n",
        "and finally a dense layer with 1 unit.\n",
        "Each block in the convolution tower consists of a RELU activation layer \n",
        "followed by 1D convolution with 128 filters with kernel width 5. \n",
        "The noise is generated by the numpy.random.normal(0, 1, (batch_size, 128)) command. We used a batch size of 128. \n",
        "For every train_on_batch iteration of the generator, we performed 10 train_on_batch iteration for the discriminator. \n",
        "We used Adam optimizer with learning_rate of 0.0001, beta_1 of 0.5, and beta_2 of 0.9. \n",
        "We trained the models for around 260,000 batch training iteration for KC and \n",
        "around 160,000 batch training iteration for MEL.\n"
      ],
      "metadata": {
        "id": "qgYRymHH8u4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "SIZE_OF_INPUT = 128\n",
        "SIZE_OF_FEATURE_MAP = 128\n",
        "DNA_BP = 200\n",
        "SIZE_OF_HIDDEN_LAYERS = DNA_BP*SIZE_OF_INPUT\n",
        "NUM_OF_1D_CONV_FILTERS = 4\n",
        "NUM_OF_CONV_1D = 5\n",
        "\n",
        "NUM_EPOCHS = 6000\n",
        "LEARNING_RATE = 0.7\n",
        "MOMENTUM = 0.9\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "a6eI8V7n7pR2"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "      \n",
        "        self.linears = nn.Sequential(\n",
        "            nn.Linear(SIZE_OF_INPUT, SIZE_OF_HIDDEN_LAYERS),\n",
        "            nn.ReLU(),  # replace ELU with RELU\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "        self.relu=nn.ReLU()\n",
        "        self.conv_1d= nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5,stride=1, padding=2, dilation=1)\n",
        "        self.final_conv_1d=nn.Conv1d(in_channels=128,out_channels=4,kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        dense_output = self.linears(x)\n",
        "        conv_1d_input = torch.reshape(dense_output, (dense_output.shape[0], SIZE_OF_INPUT, DNA_BP )) \n",
        "      \n",
        "        for i in range(NUM_OF_CONV_1D):  # a convolution tower of 5 convolution blocks with skip connections           \n",
        "            if i==0:\n",
        "              residual = conv_1d_input\n",
        "              output = F.relu(self.conv_1d(conv_1d_input))\n",
        "              output += residual\n",
        "              \n",
        "            else:\n",
        "              residual=output\n",
        "              output = F.relu(self.conv_1d(output))\n",
        "              output += residual\n",
        "       \n",
        "        output = self.final_conv_1d(output)\n",
        "        output = F.softmax(output,dim=1)\n",
        "            \n",
        "        return output"
      ],
      "metadata": {
        "id": "osiPt2bW8Vod"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Gen_test=Generator()\n",
        "normal_dist=torch.randn(BATCH_SIZE,128)\n",
        "gen_dna=Gen_test(normal_dist) # final output should be (200,4) or (4,200) matrix"
      ],
      "metadata": {
        "id": "DDfoi_Ug0CFu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accessible elements are typically around 200bp in length, and using larger regions could conflate things by combining multiple neighboring sites.\n",
        "\n",
        "For initial testing purpose, we will use @meuleman's 200 bp dna dataset"
      ],
      "metadata": {
        "id": "HcKoUTq8feDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_dna.shape"
      ],
      "metadata": {
        "id": "txjniZW1Ln2X",
        "outputId": "cc2bcb89-b5d7-479e-92f5-3f98068c4a24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([16, 4, 200])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What they say about the discriminator:\n",
        "\n",
        "\n",
        "Discriminator\t takes\t500\tbp\tone-hot\tencoded\tDNA\tsequence\tas\tinput\t(real or\tfake),\tfollowed\tby\ta\t1D\tconvolution\tlayer\twith\t128 filters\t with\tkernel\twidth\t1,\ta\tconvolution\ttower\tof\t5\tconvolution\t blocks\twith\t skip\t connections,\ta\t flatten\tlayer,\tand\t finally\ta\t  dense\tlayer\twith\t1\tunit. Each\t block\t in\t the\t convolution\t tower\t consists\t of\t a\t RELU\t activation\tlayer\tfollowed\tby\t1D\tconvolution\twith\t128\tfilters\t with\t kernel\t width\t 5.\t "
      ],
      "metadata": {
        "id": "Gfk8_IRJIRKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.first_conv = nn.Conv1d(in_channels=4,out_channels=128,kernel_size=1)\n",
        "        self.conv_1d = nn.Conv1d(in_channels=128, out_channels=128, kernel_size=5,stride=1, padding=2, dilation=1)\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(SIZE_OF_FEATURE_MAP*DNA_BP,1),\n",
        "            nn.ReLU(),  # replace ELU with RELU\n",
        "            nn.Dropout()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):       \n",
        "        conv_1d_input = self.first_conv(x)\n",
        "\n",
        "        for i in range(NUM_OF_CONV_1D):  # a convolution tower of 5 convolution blocks with skip connections\n",
        "          if i==0:\n",
        "            residual = conv_1d_input\n",
        "            output = F.relu(self.conv_1d(conv_1d_input))\n",
        "            output += residual\n",
        "\n",
        "          else:\n",
        "            residual = output\n",
        "            output = F.relu(self.conv_1d(output))\n",
        "            output += residual\n",
        "\n",
        "        output = output.view(output.shape[0],-1)\n",
        "        output = self.linear(output)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "0mXI6AfvILKk"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dis=Discriminator()\n",
        "test_input=dis(gen_dna)\n",
        "print(test_input.shape)"
      ],
      "metadata": {
        "id": "jIzGrJn5LeG7",
        "outputId": "7903babf-6775-45d3-ea2c-34c6bcf2579e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer"
      ],
      "metadata": {
        "id": "XQAtUh9n9Anp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See https://zhuanlan.zhihu.com/p/25071913 for a chinese explanation on WGAN-GP\n",
        "# Reused from https://github.com/EmilienDupont/wgan-gp/blob/ef82364f2a2ec452a52fbf4a739f95039ae76fe3/training.py\n",
        "class Trainer:\n",
        "    def __init__(self, generator, discriminator, gen_optimizer, dis_optimizer,\n",
        "                 gp_weight=10, critic_iterations=5, print_every=50,\n",
        "                 use_cuda=False):\n",
        "        self.G = generator\n",
        "        self.G_opt = gen_optimizer\n",
        "        self.D = discriminator\n",
        "        self.D_opt = dis_optimizer\n",
        "        self.losses = {'G': [], 'D': [], 'GP': [], 'gradient_norm': []}\n",
        "        self.num_steps = 0\n",
        "        self.use_cuda = use_cuda\n",
        "        self.gp_weight = gp_weight\n",
        "        self.critic_iterations = critic_iterations\n",
        "        self.print_every = print_every\n",
        "\n",
        "        if self.use_cuda:\n",
        "            self.G.cuda()\n",
        "            self.D.cuda()\n",
        "\n",
        "    def _critic_train_iteration(self, data):\n",
        "        \"\"\" \"\"\"\n",
        "        # Get generated data\n",
        "        batch_size = data.size()[0]\n",
        "        generated_data = self.sample_generator(batch_size)\n",
        "\n",
        "        # Calculate probabilities on real and generated data\n",
        "        data = Variable(data)\n",
        "        if self.use_cuda:\n",
        "            data = data.cuda()\n",
        "        d_real = self.D(data)\n",
        "        d_generated = self.D(generated_data)\n",
        "\n",
        "        # Get gradient penalty\n",
        "        gradient_penalty = self._gradient_penalty(data, generated_data)\n",
        "        self.losses['GP'].append(gradient_penalty.data[0])\n",
        "\n",
        "        # Create total loss and optimize\n",
        "        self.D_opt.zero_grad()\n",
        "        d_loss = d_generated.mean() - d_real.mean() + gradient_penalty\n",
        "        d_loss.backward()\n",
        "\n",
        "        self.D_opt.step()\n",
        "\n",
        "        # Record loss\n",
        "        self.losses['D'].append(d_loss.data[0])\n",
        "\n",
        "    def _generator_train_iteration(self, data):\n",
        "        \"\"\" \"\"\"\n",
        "        self.G_opt.zero_grad()\n",
        "\n",
        "        # Get generated data\n",
        "        batch_size = data.size()[0]\n",
        "        generated_data = self.sample_generator(batch_size)\n",
        "\n",
        "        # Calculate loss and optimize\n",
        "        d_generated = self.D(generated_data)\n",
        "        g_loss = - d_generated.mean()\n",
        "        g_loss.backward()\n",
        "        self.G_opt.step()\n",
        "\n",
        "        # Record loss\n",
        "        self.losses['G'].append(g_loss.data[0])\n",
        "\n",
        "    def _gradient_penalty(self, real_data, generated_data):\n",
        "        batch_size = real_data.size()[0]\n",
        "\n",
        "        # https://ai.stackexchange.com/questions/34926/why-do-we-use-a-linear-interpolation-of-fake-and-real-data-to-penalize-the-gradi\n",
        "        # Calculates interpolation\n",
        "        alpha = torch.rand(batch_size, 1, 1, 1)\n",
        "        alpha = alpha.expand_as(real_data)\n",
        "        if self.use_cuda:\n",
        "            alpha = alpha.cuda()\n",
        "        interpolated = alpha * real_data.data + (1 - alpha) * generated_data.data\n",
        "        interpolated = Variable(interpolated, requires_grad=True)\n",
        "        if self.use_cuda:\n",
        "            interpolated = interpolated.cuda()\n",
        "\n",
        "        # Calculate probability of interpolated examples\n",
        "        prob_interpolated = self.D(interpolated)\n",
        "\n",
        "        # Calculate gradients of probabilities with respect to examples\n",
        "        gradients = torch_grad(outputs=prob_interpolated, inputs=interpolated,\n",
        "                               grad_outputs=torch.ones(prob_interpolated.size()).cuda()\n",
        "                               if self.use_cuda else torch.ones(prob_interpolated.size()),\n",
        "                               create_graph=True, retain_graph=True)[0]\n",
        "\n",
        "        # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
        "        # so flatten to easily take norm per example in batch\n",
        "        gradients = gradients.view(batch_size, -1)\n",
        "        self.losses['gradient_norm'].append(gradients.norm(2, dim=1).mean().data[0])\n",
        "\n",
        "        # Derivatives of the gradient close to 0 can cause problems because of\n",
        "        # the square root, so manually calculate norm and add epsilon\n",
        "        gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
        "\n",
        "        # Return gradient penalty\n",
        "        return self.gp_weight * ((gradients_norm - 1) ** 2).mean()\n",
        "\n",
        "    def _train_epoch(self, data_loader):\n",
        "        for i, data in enumerate(data_loader):\n",
        "            self.num_steps += 1\n",
        "            self._critic_train_iteration(data[0])\n",
        "            # Only update generator every |critic_iterations| iterations\n",
        "            if self.num_steps % self.critic_iterations == 0:\n",
        "                self._generator_train_iteration(data[0])\n",
        "\n",
        "            if i % self.print_every == 0:\n",
        "                print(\"Iteration {}\".format(i + 1))\n",
        "                print(\"D: {}\".format(self.losses['D'][-1]))\n",
        "                print(\"GP: {}\".format(self.losses['GP'][-1]))\n",
        "                print(\"Gradient norm: {}\".format(self.losses['gradient_norm'][-1]))\n",
        "                if self.num_steps > self.critic_iterations:\n",
        "                    print(\"G: {}\".format(self.losses['G'][-1]))\n",
        "\n",
        "    def train(self, data_loader, epochs, save_training_gif=True):\n",
        "        if save_training_gif:\n",
        "            # Fix latents to see how image generation improves during training\n",
        "            fixed_latents = Variable(self.G.sample_latent(64))\n",
        "            if self.use_cuda:\n",
        "                fixed_latents = fixed_latents.cuda()\n",
        "            training_progress_images = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print(\"\\nEpoch {}\".format(epoch + 1))\n",
        "            self._train_epoch(data_loader)\n",
        "\n",
        "            if save_training_gif:\n",
        "                # Generate batch of images and convert to grid\n",
        "                img_grid = make_grid(self.G(fixed_latents).cpu().data)\n",
        "                # Convert to numpy and transpose axes to fit imageio convention\n",
        "                # i.e. (width, height, channels)\n",
        "                img_grid = np.transpose(img_grid.numpy(), (1, 2, 0))\n",
        "                # Add image grid to training progress\n",
        "                training_progress_images.append(img_grid)\n",
        "\n",
        "        if save_training_gif:\n",
        "            imageio.mimsave('./training_{}_epochs.gif'.format(epochs),\n",
        "                            training_progress_images)\n",
        "\n",
        "    def sample_generator(self, num_samples):\n",
        "        latent_samples = Variable(self.G.sample_latent(num_samples))\n",
        "        if self.use_cuda:\n",
        "            latent_samples = latent_samples.cuda()\n",
        "        generated_data = self.G(latent_samples)\n",
        "        return generated_data\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        generated_data = self.sample_generator(num_samples)\n",
        "        # Remove color channel\n",
        "        return generated_data.data.cpu().numpy()[:, 0, :, :]\n"
      ],
      "metadata": {
        "id": "BeNX4Vf58-Yh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing network"
      ],
      "metadata": {
        "id": "s35JFFAl9CXS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LKIlIaMp9Gcb"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}